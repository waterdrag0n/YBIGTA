{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제1: tensorflow를 이용한 NN 밑바닥부터 구현\n",
    "input feature가 100개이고,  \n",
    "hidden layer가 2개이고 neuron이 각각 50,10개이고,  \n",
    "output이 5개인 NN를 구현해 보자  \n",
    "* hidden layer는 relu를 activation function으로, output layer는 softmax를 activation function으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 100\n",
    "n_h1 = 50\n",
    "n_h2 = 10\n",
    "n_y = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.6722908   1.4536468  -0.9299077  ... -0.30275846 -0.3271646\n",
      "   0.09738334]\n",
      " [ 0.6872283  -1.1724182   0.69369215 ... -0.23034436  0.42302588\n",
      "  -0.2671203 ]\n",
      " [ 1.4488789  -0.26608607  0.5757674  ... -0.747439   -0.6232004\n",
      "  -1.3964397 ]\n",
      " ...\n",
      " [-1.5657307  -0.4636423  -0.80649847 ...  1.058284    0.41295722\n",
      "   0.03121383]\n",
      " [-1.1301254   0.04605759 -0.19594407 ...  1.1675403   0.05958118\n",
      "  -0.33428204]\n",
      " [ 0.25861773  1.3405094   0.5092561  ... -0.4376483  -2.1302671\n",
      "   0.61373043]]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Simulate train set\n",
    "m = 500\n",
    "\n",
    "x_train=np.random.randn(m,n_x).astype(np.float32)\n",
    "y_train=np.zeros((m,n_y)).astype(np.float32)\n",
    "y_train[np.arange(m),np.random.randint(n_y,size=m)]=1\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialization of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=tf.Variable(1e-3*np.random.randn(n_x,n_h1).astype(np.float32),name=\"w1\")\n",
    "## 코드를 작성해 보세요 ##\n",
    "w2=tf.Variable(1e-3*np.random.randn(n_h1, n_h2).astype(np.float32), name=\"w2\")\n",
    "w3= tf.Variable(1e-3*np.random.randn(n_h2, n_y).astype(np.float32), name=\"w3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* forward propagation을 통해 prediction 값을 구하고, loss를 구하는 function을 만들어 봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    z1=tf.matmul(x,w1)\n",
    "    a1=tf.nn.relu(z1)\n",
    "    ## 코드를 작성해 보세요 ##\n",
    "    z2=tf.matmul(a1, w2)\n",
    "    a2=tf.nn.relu(z2)\n",
    "    z3=tf.matmul(a2, w3)\n",
    "    predictions = tf.nn.softmax(z3)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def loss_fn(predictions, y):\n",
    "    loss= -tf.reduce_sum(y*tf.math.log(predictions))\n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* backpropagation & update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-2\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = forward(x)\n",
    "        loss = loss_fn(predictions, y)\n",
    "    ## 코드를 작성해 보세요 ## (hint: tape.gradient를 구글링 해보세요)\n",
    "    gradient = tape.gradient(loss, [w1, w2])\n",
    "    # optimizer와 위에서 구한 경사도를 이용해 가중치들을 업데이트 합니다.\n",
    "    optimizer.apply_gradients(zip(gradient, [w1, w2]))\n",
    "    return loss, w1, w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 간단하게 train loop를 작성해 loss가 줄어나가는지 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[804.71875, 804.7179, 804.70416, 804.6771, 804.6314, 804.5641, 804.47266, 804.35455, 804.20825, 804.03174]\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "\n",
    "for step in range(10): \n",
    "    loss, w1, w2 = train_step(x_train, y_train)\n",
    "    loss_list.append(loss.numpy())\n",
    "    \n",
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제2: MNIST 데이터를 나만의 NN model로 95 % 이상의 성능으로 training 시켜보자!\n",
    "\n",
    "\n",
    "## Loading MNIST training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Loading the data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scaling(image data는 min-max scaling 주로 사용)\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "28 * 28 pixel 값을 가진 총 60000개의 이미지 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network 모델에 맞게 이미지 데이터를 벡터 형태로 데이터를 reshape 합니다.  \n",
    "(Model을 만들 때 *keras.layers.Flatten(input_shape=(28, 28)) 이용해도 됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test = x_train.reshape((-1, 28*28)), x_test.reshape((-1, 28*28))\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0].reshape(28,28,1)).set_cmap('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Labels\n",
    "이미지 데이터가 나타내는 숫자값을 label로 가지고 있고, 0부터 9까지의 값을 나타냄  \n",
    "마찬가지로, 60000개의 label이 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show MNIST label for above data\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나만의 모델을 tensorflow keras API 를 이용해 만들어 봅시다~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parameters for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_list = [\"sigmoid\", \"relu\", \"softmax\", \"tanh\"]\n",
    "\n",
    "loss_list = [\"sparse_categorical_crossentropy\",\n",
    "             \"categorical_crossentropy\", \n",
    "             \"binary_crossentropy\"]\n",
    "\n",
    "optimizer_list = [\"sgd\", \"adam\", \"rmsprop\", \"adagrad\"]\n",
    "\n",
    "initializer_list = [tf.keras.initializers.RandomNormal(), \n",
    "                    tf.keras.initializers.RandomUniform(), \n",
    "                    tf.keras.initializers.he_normal(), \n",
    "                    tf.keras.initializers.he_uniform(), \n",
    "                    tf.keras.initializers.GlorotUniform(),\n",
    "                    tf.keras.initializers.GlorotNormal()]\n",
    "\n",
    "# dropout\n",
    "dropout_rate = 0.3\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_dim=784, activation = \"sigmoid\"),\n",
    "    tf.keras.layers.Dense(2, activation = \"sigmoid\"),\n",
    "    tf.keras.layers.Dropout(dropout_rate)\n",
    "])\n",
    "\n",
    "\n",
    "# regularizer\n",
    "regularizer = tf.keras.regularizers.l1(1e-3)\n",
    "regularizer = tf.keras.regularizers.l2(1e-3)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_dim=784, activation=\"sigmoid\",\n",
    "                          activity_regularizer=regularizer)\n",
    "])\n",
    "\n",
    "# weight initialization\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_dim=784, activation=\"sigmoid\",\n",
    "                          kernel_initializer=initializer_list[0])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Own Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 자유롭게 Model을 만들고 compile 해봅시다 ####\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_dim=784, activation='relu'),\n",
    "    tf.keras.layers.Dropout(dropout_rate),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(dropout_rate),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(dropout_rate),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(dropout_rate),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내가 만든 모델을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model을 자유롭게 train 해봅시다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.8838 - accuracy: 0.7080\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2517 - accuracy: 0.9368\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2040 - accuracy: 0.9499\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1673 - accuracy: 0.9582\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1453 - accuracy: 0.9639\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1330 - accuracy: 0.9676\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1209 - accuracy: 0.9698\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1093 - accuracy: 0.9718\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1085 - accuracy: 0.9730\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0996 - accuracy: 0.9740\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0930 - accuracy: 0.9754\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0829 - accuracy: 0.9787\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0901 - accuracy: 0.9774\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0823 - accuracy: 0.9792\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0766 - accuracy: 0.9794\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0768 - accuracy: 0.9809\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0732 - accuracy: 0.9805\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0696 - accuracy: 0.9826: 0s - loss: 0.0\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0677 - accuracy: 0.9831\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0660 - accuracy: 0.9830\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0644 - accuracy: 0.9835\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0604 - accuracy: 0.9845\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0621 - accuracy: 0.9835\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0540 - accuracy: 0.9857\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0563 - accuracy: 0.9849\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0546 - accuracy: 0.9861\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0492 - accuracy: 0.9869\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0558 - accuracy: 0.9855\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0534 - accuracy: 0.9871\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0576 - accuracy: 0.9861\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0506 - accuracy: 0.9877\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0493 - accuracy: 0.9876\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0479 - accuracy: 0.9881\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0463 - accuracy: 0.9875\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0490 - accuracy: 0.9873: 0s - loss: 0.0490 - accura - ETA: 0s - loss: 0.0490 - accuracy: 0.\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0479 - accuracy: 0.9878\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0488 - accuracy: 0.9885\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0452 - accuracy: 0.9879\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0455 - accuracy: 0.9878\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0460 - accuracy: 0.9890\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0422 - accuracy: 0.9895\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0443 - accuracy: 0.9888\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0404 - accuracy: 0.9894\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0444 - accuracy: 0.9887\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0436 - accuracy: 0.9895\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0465 - accuracy: 0.9888\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0372 - accuracy: 0.9897\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0377 - accuracy: 0.9897\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0404 - accuracy: 0.9901\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0354 - accuracy: 0.9911\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0354 - accuracy: 0.9904\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0423 - accuracy: 0.9905\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0353 - accuracy: 0.9913\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0384 - accuracy: 0.9917\n",
      "Epoch 55/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0378 - accuracy: 0.9906\n",
      "Epoch 56/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0357 - accuracy: 0.9913\n",
      "Epoch 57/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0364 - accuracy: 0.9908\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0361 - accuracy: 0.9916\n",
      "Epoch 59/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0332 - accuracy: 0.9912\n",
      "Epoch 60/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0349 - accuracy: 0.9917\n",
      "Epoch 61/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0356 - accuracy: 0.9913\n",
      "Epoch 62/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0337 - accuracy: 0.9917\n",
      "Epoch 63/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0335 - accuracy: 0.9913\n",
      "Epoch 64/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0315 - accuracy: 0.9918\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0294 - accuracy: 0.9927\n",
      "Epoch 66/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0338 - accuracy: 0.9925\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0336 - accuracy: 0.9915\n",
      "Epoch 68/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0309 - accuracy: 0.9922\n",
      "Epoch 69/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0318 - accuracy: 0.9922\n",
      "Epoch 70/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0330 - accuracy: 0.9926\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0313 - accuracy: 0.9921\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0316 - accuracy: 0.9918\n",
      "Epoch 73/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0369 - accuracy: 0.9914\n",
      "Epoch 74/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0331 - accuracy: 0.9923\n",
      "Epoch 75/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0299 - accuracy: 0.9924\n",
      "Epoch 76/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0355 - accuracy: 0.9917\n",
      "Epoch 77/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0348 - accuracy: 0.9920\n",
      "Epoch 78/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0299 - accuracy: 0.9923\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0320 - accuracy: 0.9923\n",
      "Epoch 80/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0276 - accuracy: 0.9934\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0280 - accuracy: 0.9935\n",
      "Epoch 82/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0273 - accuracy: 0.9930\n",
      "Epoch 83/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0317 - accuracy: 0.9927\n",
      "Epoch 84/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0310 - accuracy: 0.9927\n",
      "Epoch 85/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0281 - accuracy: 0.9931\n",
      "Epoch 86/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0291 - accuracy: 0.9932\n",
      "Epoch 87/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0253 - accuracy: 0.9933\n",
      "Epoch 88/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0321 - accuracy: 0.9920\n",
      "Epoch 89/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0350 - accuracy: 0.9919\n",
      "Epoch 90/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0261 - accuracy: 0.9933\n",
      "Epoch 91/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0299 - accuracy: 0.9928\n",
      "Epoch 92/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0250 - accuracy: 0.9935\n",
      "Epoch 93/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0321 - accuracy: 0.9929\n",
      "Epoch 94/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0322 - accuracy: 0.9929\n",
      "Epoch 95/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0291 - accuracy: 0.9929\n",
      "Epoch 96/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0263 - accuracy: 0.9936\n",
      "Epoch 97/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0280 - accuracy: 0.9931\n",
      "Epoch 98/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0300 - accuracy: 0.9931\n",
      "Epoch 99/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0264 - accuracy: 0.9934\n",
      "Epoch 100/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0275 - accuracy: 0.9931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22c79e9ac10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95%이상의 성능을 가진 모델을 만들면 완성!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.1707 - accuracy: 0.9839\n",
      "\n",
      "Accuracy: 0.9839000105857849\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test,y_test, verbose=2)\n",
    "\n",
    "print('\\nAccuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](https://www.tensorflow.org/versions/master/images/mnist_tensorboard.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
